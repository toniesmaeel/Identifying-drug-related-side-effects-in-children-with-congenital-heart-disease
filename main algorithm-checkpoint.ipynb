# --- Imports ---
import pandas as pd
import numpy as np
import os
import warnings
import plotly.graph_objects as go
import plotly.express as px
from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_curve, auc, confusion_matrix, precision_recall_curve
from sklearn.feature_selection import mutual_info_classif, RFECV
from sklearn.feature_selection import VarianceThreshold
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
import plotly.figure_factory as ff
import seaborn as sns
import matplotlib.pyplot as plt
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit.DataStructs import TanimotoSimilarity
import umap
import scipy.cluster.hierarchy as sch
from scipy.spatial.distance import pdist, squareform
import shap

# --- Suppress Warnings ---
warnings.filterwarnings('ignore')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# --- Drug-side effect mapping (from Table 1) ---
drug_side_effects = {
    'Amiodarone': ['Bradycardia', 'Hypotension', 'Hypothyroidism'],
    'Aspirin': ['Hematochezia'],
    'Atenolol': ['Palpitations', 'Asthma', 'Vertigo', 'Cold extremities'],
    'Captopril': ['Hypotension'],
    'Dexmedetomidine': ['Bradycardia'],
    'Sotalol': ['Bradycardia'],
    'Flecainide': ['Ventricular dysfunction'],
    'Furosemide': ['Hypokalemia', 'Hypovolemia', 'Bone fractures'],
    'Heparin': ['Thrombocytopenia', 'Postoperative bleeding'],
    'Indomethacin': ['Necrotizing enterocolitis', 'Gastrointestinal perforation', 'Oliguria',
                     'Anuria', 'Gastrointestinal hemorrhage', 'Intracerebral hemorrhage',
                     'Elevation of serum creatinine', 'Thrombocytopenia'],
    'Ibuprofen': ['Necrotizing enterocolitis', 'Tachypnoea', 'Retinopathy of prematurity',
                  'Intraventricular hemorrhage', 'Gastrointestinal hemorrhage'],
    'Iloprost': ['Facial flushing'],
    'Sildenafil': ['Facial flushing'],
    'Tadalafil': ['Headache'],
    'Prostaglandin E1': ['Apnea', 'Hypoventilation', 'Fever', 'Hyperthermia', 'Facial flushing']
}

# --- Multi-domain feature integration strategies (18 strategies as described) ---
feature_combinations = {
    'Clinical Only': ['clinical'],
    'Clinical+Chemical': ['clinical', 'chemical'],
    'Clinical+Biological': ['clinical', 'biological'],
    'Clinical+Phenotypic': ['clinical', 'phenotypic'],
    'Clinical+Chemical+Biological': ['clinical', 'chemical', 'biological'],
    'Clinical+Chemical+Phenotypic': ['clinical', 'chemical', 'phenotypic'],
    'Clinical+Biological+Phenotypic': ['clinical', 'biological', 'phenotypic'],
    'Clinical+Chemical+Biological+Phenotypic': ['clinical', 'chemical', 'biological', 'phenotypic'],
    'Clinical+Target': ['clinical', 'target'],
    'Clinical+Enzyme': ['clinical', 'enzyme'],
    'Clinical+Pathway': ['clinical', 'pathway'],
    'Clinical+Target+Enzyme': ['clinical', 'target', 'enzyme'],
    'Clinical+Target+Pathway': ['clinical', 'target', 'pathway'],
    'Clinical+Enzyme+Pathway': ['clinical', 'enzyme', 'pathway'],
    'Clinical+Therapeutic': ['clinical', 'therapeutic'],
    'Clinical+Phenotype_Class': ['clinical', 'phenotype_class'],
    'Clinical+Therapeutic+Phenotype_Class': ['clinical', 'therapeutic', 'phenotype_class'],
    'Clinical+Integrated_Cluster': ['clinical', 'integrated_cluster']
}

class MultiDomainDrugAnalyzer:
    def __init__(self):
        self.drug_descriptors = {}
        self.domain_clusters = {}
        self.integrated_clusters = {}
        
    def compute_chemical_fingerprints(self, smiles_dict):
        """Compute ECFP4 fingerprints for drugs"""
        fingerprints = {}
        for drug, smiles in smiles_dict.items():
            try:
                mol = Chem.MolFromSmiles(smiles)
                if mol:
                    fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)
                    fingerprints[drug] = fp
                else:
                    print(f"Could not process SMILES for {drug}: {smiles}")
                    fingerprints[drug] = None
            except Exception as e:
                print(f"Error processing {drug}: {e}")
                fingerprints[drug] = None
        return fingerprints
    
    def compute_chemical_similarity(self, fp1, fp2):
        """Compute Tanimoto coefficient between two fingerprints"""
        if fp1 is None or fp2 is None:
            return 0.0
        return TanimotoSimilarity(fp1, fp2)
    
    def compute_chemical_complexity(self, drug_properties):
        """Compute chemical complexity score Î¾j"""
        complexity_scores = {}
        for drug, props in drug_properties.items():
            # Normalize components (min-max scaling)
            mw_norm = (props['molecular_weight'] - props['mw_min']) / (props['mw_max'] - props['mw_min'])
            ring_norm = (props['ring_count'] - props['ring_min']) / (props['ring_max'] - props['ring_min'])
            aromatic_norm = props['aromatic_fraction']  # Already 0-1
            halogen_norm = (props['halogen_count'] - props['halogen_min']) / (props['halogen_max'] - props['halogen_min'])
            sulfur_norm = (props['sulfur_count'] - props['sulfur_min']) / (props['sulfur_max'] - props['sulfur_min'])
            
            # Standardized combination (coefficients scaled to unit variance)
            xi_j = (mw_norm + ring_norm + aromatic_norm + halogen_norm + sulfur_norm) / 5.0
            complexity_scores[drug] = xi_j
            
        return complexity_scores
    
    def compute_jaccard_similarity(self, vec1, vec2):
        """Compute Jaccard similarity for binary vectors"""
        intersection = np.logical_and(vec1, vec2).sum()
        union = np.logical_or(vec1, vec2).sum()
        return intersection / union if union != 0 else 0.0
    
    def perform_domain_clustering(self, similarity_matrix, linkage_method='ward'):
        """Perform hierarchical clustering on domain similarity matrix"""
        dissimilarity = 1 - similarity_matrix
        linkage_matrix = sch.linkage(squareform(dissimilarity), method=linkage_method)
        return linkage_matrix
    
    def create_integrated_descriptors(self, chemical_fps, biological_vectors, phenotypic_vectors):
        """Create integrated multi-domain descriptors"""
        integrated_descriptors = {}
        
        for drug in chemical_fps.keys():
            if drug in biological_vectors and drug in phenotypic_vectors:
                # Normalize and concatenate domain vectors
                chem_vec = np.array(chemical_fps[drug]) if chemical_fps[drug] is not None else np.zeros(2048)
                bio_vec = biological_vectors[drug] / np.linalg.norm(biological_vectors[drug]) if np.linalg.norm(biological_vectors[drug]) > 0 else biological_vectors[drug]
                pheno_vec = phenotypic_vectors[drug] / np.linalg.norm(phenotypic_vectors[drug]) if np.linalg.norm(phenotypic_vectors[drug]) > 0 else phenotypic_vectors[drug]
                
                # L2 normalize chemical fingerprints for integration
                chem_vec_norm = chem_vec / np.linalg.norm(chem_vec) if np.linalg.norm(chem_vec) > 0 else chem_vec
                
                integrated_descriptors[drug] = np.concatenate([chem_vec_norm, bio_vec, pheno_vec])
                
        return integrated_descriptors

def load_and_preprocess_data():
    """Load and preprocess data according to methodology"""
    filepath = 'balanced_dataset2.xlsx'
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"Data file not found at: {filepath}")
    
    df = pd.read_excel(filepath)
    df = df.dropna(subset=['Drug name'])
    
    # Data quality control - exclude records with >20% missing data
    missing_threshold = len(df.columns) * 0.2
    df = df.dropna(thresh=len(df.columns) - missing_threshold)
    
    # Handle missing values
    for column in df.columns:
        if df[column].dtype in ['int64', 'float64']:
            # Continuous variables with <=10% missingness
            if df[column].isnull().mean() <= 0.1:
                df[column].fillna(df[column].median(), inplace=True)
        else:
            # Categorical variables - impute with mode
            if df[column].isnull().mean() <= 0.1:
                df[column].fillna(df[column].mode()[0] if not df[column].mode().empty else 'Unknown', inplace=True)
    
    # Create derived features
    if 'Age' in df.columns and 'Weight' in df.columns:
        df['Age_Weight_Ratio'] = df['Age'] / (df['Weight'].replace(0, 0.1))
    
    # Normalize continuous features using min-max scaling
    continuous_features = ['Age', 'Weight', 'Age_Weight_Ratio'] if 'Age_Weight_Ratio' in df.columns else ['Age', 'Weight']
    for feature in continuous_features:
        if feature in df.columns:
            scaler = MinMaxScaler()
            df[feature] = scaler.fit_transform(df[[feature]])
    
    return df

def three_stage_feature_selection(X, y, n_features=20):
    """Three-stage feature selection process"""
    # Stage 1: Remove near-zero variance features (â‰¤0.01)
    variance_selector = VarianceThreshold(threshold=0.01)
    X_variance = variance_selector.fit_transform(X)
    selected_features_variance = X.columns[variance_selector.get_support()]
    
    # Stage 2: Mutual information - top 100 features
    if len(selected_features_variance) > 100:
        mi_scores = mutual_info_classif(X_variance, y, random_state=42)
        top_100_idx = np.argsort(mi_scores)[-100:]
        selected_features_mi = selected_features_variance[top_100_idx]
        X_mi = X_variance[:, top_100_idx]
    else:
        selected_features_mi = selected_features_variance
        X_mi = X_variance
    
    # Stage 3: RFECV with Random Forest
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rfecv = RFECV(estimator=rf, cv=StratifiedKFold(3), scoring='roc_auc', n_jobs=-1)
    rfecv.fit(X_mi, y)
    
    # Select final n_features
    if hasattr(rfecv, 'ranking_'):
        final_feature_idx = np.where(rfecv.ranking_ <= n_features)[0]
        if len(final_feature_idx) < n_features:
            final_feature_idx = np.argsort(rfecv.ranking_)[:n_features]
    else:
        final_feature_idx = np.arange(min(n_features, X_mi.shape[1]))
    
    final_features = selected_features_mi[final_feature_idx]
    
    return final_features, X_mi[:, final_feature_idx]

def create_model_pipeline(model, numeric_features, categorical_features):
    """Create preprocessing pipeline with feature selection"""
    preprocessor = ColumnTransformer([
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])
    
    return ImbPipeline([
        ('preprocessor', preprocessor),
        ('smote', SMOTE(random_state=42, k_neighbors=5)),
        ('classifier', model)
    ])

def nested_cross_validation(X, y, model, param_grid, inner_cv=5, outer_cv=5):
    """Perform nested cross-validation with hyperparameter tuning"""
    outer_cv = StratifiedKFold(n_splits=outer_cv, shuffle=True, random_state=42)
    inner_cv = StratifiedKFold(n_splits=inner_cv, shuffle=True, random_state=42)
    
    best_score = 0
    best_params = None
    best_model = None
    
    for train_idx, test_idx in outer_cv.split(X, y):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
        
        # Feature selection on training fold
        selected_features, X_train_selected = three_stage_feature_selection(X_train, y_train)
        X_test_selected = X_test[selected_features]
        
        # Hyperparameter tuning
        grid_search = GridSearchCV(model, param_grid, cv=inner_cv, scoring='roc_auc', n_jobs=-1)
        grid_search.fit(X_train_selected, y_train)
        
        # Evaluate on test fold
        score = grid_search.score(X_test_selected, y_test)
        
        if score > best_score:
            best_score = score
            best_params = grid_search.best_params_
            best_model = grid_search.best_estimator_
    
    return best_model, best_params, best_score

def evaluate_models_comprehensive(X, y, drug, side_effect, feature_name):
    """Comprehensive model evaluation with all performance metrics"""
    # Split data with stratification
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=42
    )
    
    # Feature selection on training data only
    selected_features, X_train_selected = three_stage_feature_selection(X_train, y_train)
    X_test_selected = X_test[selected_features]
    
    numeric_features = X_train_selected.select_dtypes(include=['int64', 'float64']).columns.tolist()
    categorical_features = X_train_selected.select_dtypes(include=['object', 'category']).columns.tolist()
    
    # Define models with comprehensive parameter grids
    models = {
        'Logistic Regression': {
            'model': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),
            'params': {
                'classifier__C': [0.1, 1, 10],
                'classifier__penalty': ['l2', 'none']
            }
        },
        'SVM': {
            'model': SVC(probability=True, random_state=42, class_weight='balanced'),
            'params': {
                'classifier__C': [0.1, 1, 10],
                'classifier__gamma': ['scale', 'auto'],
                'classifier__kernel': ['rbf']
            }
        },
        'Random Forest': {
            'model': RandomForestClassifier(random_state=42, class_weight='balanced'),
            'params': {
                'classifier__n_estimators': [100, 200],
                'classifier__max_depth': [10, 20, None],
                'classifier__min_samples_leaf': [1, 2]
            }
        },
        'KNN': {
            'model': KNeighborsClassifier(),
            'params': {
                'classifier__n_neighbors': [3, 5, 7],
                'classifier__weights': ['uniform', 'distance']
            }
        },
        'MLP': {
            'model': MLPClassifier(random_state=42, early_stopping=True, validation_fraction=0.1),
            'params': {
                'classifier__hidden_layer_sizes': [(128,), (128, 64), (128, 64, 32)],
                'classifier__alpha': [0.0001, 0.001],
                'classifier__learning_rate_init': [0.001]
            }
        }
    }
    
    fig = go.Figure()
    colors = px.colors.qualitative.Set1
    metrics_table = []
    best_models = {}
    
    for i, (name, model_config) in enumerate(models.items()):
        try:
            print(f"        Training {name}...")
            
            # Create pipeline
            pipeline = create_model_pipeline(model_config['model'], numeric_features, categorical_features)
            
            # Hyperparameter tuning with randomized search
            from sklearn.model_selection import RandomizedSearchCV
            random_search = RandomizedSearchCV(
                pipeline, model_config['params'], 
                n_iter=20, cv=5, scoring='roc_auc', 
                random_state=42, n_jobs=-1
            )
            
            random_search.fit(X_train_selected, y_train)
            best_pipeline = random_search.best_estimator_
            best_models[name] = best_pipeline
            
            # Predictions
            y_pred = best_pipeline.predict(X_test_selected)
            y_score = best_pipeline.predict_proba(X_test_selected)[:, 1]
            
            # Calculate metrics
            fpr, tpr, _ = roc_curve(y_test, y_score)
            roc_auc = auc(fpr, tpr)
            
            tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
            accuracy = (tp + tn) / (tp + tn + fp + fn)
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = sensitivity
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            metrics_table.append([
                name, sensitivity, specificity, accuracy, 
                precision, recall, f1, roc_auc
            ])
            
            # Add to ROC plot
            fig.add_trace(go.Scatter(
                x=fpr, y=tpr, mode='lines',
                name=f'{name} (AUC={roc_auc:.3f})',
                line=dict(color=colors[i % len(colors)], width=2)
            ))
            
            print(f"        {name}: AUC = {roc_auc:.3f}, F1 = {f1:.3f}")
            
        except Exception as e:
            print(f"        Error with {name}: {str(e)}")
            continue
    
    # Add random classifier line
    fig.add_trace(go.Scatter(
        x=[0, 1], y=[0, 1], mode='lines',
        name='Random (AUC=0.500)', line=dict(color='black', dash='dash')
    ))
    
    # Update plot layout
    fig.update_layout(
        title=f'{drug} - {side_effect} [{feature_name}]',
        xaxis_title='False Positive Rate',
        yaxis_title='True Positive Rate',
        legend=dict(orientation='h', y=-0.3),
        template='plotly_white',
        height=500,
        width=700
    )
    
    # Save plot
    out_name = f'AUROC_{drug}_{side_effect}_{feature_name}.html'.replace(" ", "_")
    fig.write_html(out_name)
    print(f"        Saved ROC plot to: {out_name}")
    
    # Create metrics table
    metrics_df = pd.DataFrame(
        metrics_table, 
        columns=['Model', 'Sensitivity', 'Specificity', 'Accuracy', 
                'Precision', 'Recall', 'F1 Score', 'AUROC']
    )
    
    print("\nPerformance Metrics:")
    print(metrics_df.round(3))
    
    # Create performance heatmap
    heatmap_data = metrics_df.set_index('Model')[['AUROC', 'F1 Score', 'Accuracy', 'Precision', 'Recall']]
    heatmap_fig = ff.create_annotated_heatmap(
        z=heatmap_data.values,
        x=heatmap_data.columns.tolist(),
        y=heatmap_data.index.tolist(),
        colorscale='Blues',
        showscale=True,
        annotation_text=np.round(heatmap_data.values, 3)
    )
    heatmap_fig.update_layout(
        title_text=f'Performance Heatmap: {drug} - {side_effect}',
        margin=dict(t=50, l=100)
    )
    heatmap_fig.write_html(f"Performance_Heatmap_{drug}_{side_effect}_{feature_name}.html".replace(" ", "_"))
    
    # Feature importance analysis for best model (Random Forest)
    if 'Random Forest' in best_models:
        try:
            rf_pipeline = best_models['Random Forest']
            rf_model = rf_pipeline.named_steps['classifier']
            
            # Get feature names after preprocessing
            preprocessor = rf_pipeline.named_steps['preprocessor']
            feature_names = []
            
            # Numerical features
            if 'num' in preprocessor.named_transformers_:
                feature_names.extend(numeric_features)
            
            # Categorical features (after one-hot encoding)
            if 'cat' in preprocessor.named_transformers_:
                cat_encoder = preprocessor.named_transformers_['cat']
                if hasattr(cat_encoder, 'get_feature_names_out'):
                    cat_features = cat_encoder.get_feature_names_out(categorical_features)
                    feature_names.extend(cat_features)
                else:
                    feature_names.extend(categorical_features)
            
            # SHAP analysis
            explainer = shap.TreeExplainer(rf_model)
            shap_values = explainer.shap_values(X_test_selected)
            
            # Plot SHAP summary
            if len(shap_values) == 2:  # Binary classification
                shap_summary = shap.summary_plot(shap_values[1], X_test_selected, 
                                               feature_names=feature_names[:X_test_selected.shape[1]], 
                                               show=False)
                plt.title(f'SHAP Summary: {drug} - {side_effect}')
                plt.tight_layout()
                plt.savefig(f'SHAP_{drug}_{side_effect}_{feature_name}.png', dpi=300, bbox_inches='tight')
                plt.close()
                
        except Exception as e:
            print(f"        SHAP analysis failed: {e}")
    
    return metrics_df, best_models

def main():
    """Main execution function"""
    print("ðŸš€ Starting Multi-Domain Drug Side Effect Analysis...")
    
    # Initialize multi-domain analyzer
    analyzer = MultiDomainDrugAnalyzer()
    
    # Load and preprocess data
    df = load_and_preprocess_data()
    print("âœ… Data loaded and preprocessed. Shape:", df.shape)
    
    # Perform analysis for each drug and side effect
    for drug, side_effects in drug_side_effects.items():
        print(f"\nðŸ”¬ Analyzing Drug: {drug}")
        
        for side_effect in side_effects:
            print(f"  â–¶ Side Effect: {side_effect}")
            
            for feature_name, feature_groups in feature_combinations.items():
                print(f"    ðŸ”¹ Feature Set: {feature_name}")
                
                try:
                    # Preprocess data for current drug and side effect
                    drug_data = df[df['Drug name'] == drug].copy()
                    
                    if side_effect not in drug_data.columns:
                        print(f"    âš  Side effect '{side_effect}' not found in data. Skipping.")
                        continue
                    
                    # Define available features based on methodology
                    available_features = {
                        'clinical': ['Age', 'Sex', 'Weight', 'Comorbidity', 'Polypharmacy'],
                        'chemical': ['Chemical_Complexity', 'Molecular_Weight'] if 'Chemical_Complexity' in drug_data.columns else [],
                        'biological': ['Target_Class', 'Enzyme_Class', 'Pathway_Class'] if 'Target_Class' in drug_data.columns else [],
                        'phenotypic': ['Therapeutic_Class', 'Phenotype_Class'] if 'Therapeutic_Class' in drug_data.columns else [],
                        'target': ['Target_Class'] if 'Target_Class' in drug_data.columns else [],
                        'enzyme': ['Enzyme_Class'] if 'Enzyme_Class' in drug_data.columns else [],
                        'pathway': ['Pathway_Class'] if 'Pathway_Class' in drug_data.columns else [],
                        'therapeutic': ['Therapeutic_Class'] if 'Therapeutic_Class' in drug_data.columns else [],
                        'phenotype_class': ['Phenotype_Class'] if 'Phenotype_Class' in drug_data.columns else [],
                        'integrated_cluster': ['Integrated_Cluster'] if 'Integrated_Cluster' in drug_data.columns else []
                    }
                    
                    # Select features based on current strategy
                    selected_features = []
                    for group in feature_groups:
                        if group in available_features:
                            selected_features.extend([f for f in available_features[group] if f in drug_data.columns])
                    
                    # Add clinical features to all strategies
                    if 'clinical' not in feature_groups:
                        selected_features.extend(available_features['clinical'])
                    
                    # Remove duplicates
                    selected_features = list(set(selected_features))
                    
                    if not selected_features:
                        print(f"    âš  No valid features found for strategy: {feature_name}")
                        continue
                    
                    X = drug_data[selected_features]
                    y = drug_data[side_effect].astype(int)
                    
                    if len(y.unique()) < 2:
                        print(f"    âš  Insufficient class variety for '{side_effect}'. Skipping.")
                        continue
                    
                    # Evaluate models
                    metrics_df, best_models = evaluate_models_comprehensive(
                        X, y, drug, side_effect, feature_name
                    )
                    
                    # Save results
                    results_file = f"Results_{drug}_{side_effect}_{feature_name}.csv".replace(" ", "_")
                    metrics_df.to_csv(results_file, index=False)
                    print(f"    ðŸ’¾ Results saved to: {results_file}")
                    
                except Exception as e:
                    print(f"    âŒ Error processing {drug}-{side_effect}-{feature_name}: {str(e)}")
                    continue
    
    print("\nðŸŽ¯ Analysis completed!")

if __name__ == '__main__':
    main()
